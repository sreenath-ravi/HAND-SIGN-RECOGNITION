# Mount Google Drive to Colab notebook
from google.colab import drive
drive.mount('/content/gdrive')
 
# Here we import everything we need for the project
%matplotlib inline
from google.colab import files
import os
os.chdir('gdrive/My Drive/kaggle')
 
# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
 
# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
import cv2
import pandas as pd
 
# Sklearn
# Helps with organizing data for training
from sklearn.model_selection import train_test_split 
# Helps present results as a confusion-matrix
from sklearn.metrics import confusion_matrix 
 
print(tf.__version__)
from google.colab import files
files.upload()  #this will prompt you to upload the kaggle.json
!pip install -q kaggle 
!mkdir -p ~/.kaggle # create new directory called kaggle
!cp kaggle.json ~/.kaggle/ # copy json file to kaggle folder 
!ls ~/.kaggle 
!chmod 600 /root/.kaggle/kaggle.json  # set permission
!kaggle datasets download -d gti-upm/leapgestrecog -p /content/gdrive/My\ Drive/kaggle # download dataset
 
# Unzip images
!unzip leapgestrecog.zip
 
# We need to get all the paths for the images to later load them
imagepaths = []
 
# Go through all the files and subdirectories inside a folder and save path to images inside list
for root, dirs, files in os.walk(".", topdown=False): 
  for name in files:
    path = os.path.join(root, name)
    if path.endswith("png"): # We want only the images
      imagepaths.append(path)
 
print(len(imagepaths)) # If > 0, then a PNG image was loaded
# This function is used more for debugging and showing results later
# It plots the image into the notebook
 
# define function to display an image in a given path
def plot_image(path):
  img = cv2.imread(path) # Reads the image into a numpy.array
  # Converts RGB to greyscale
  img_cvt = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   print(img_cvt.shape)
  # Prints the shape of the image
  plt.grid(False) # Turn off grid
  plt.imshow(img_cvt) # Shows the image
  plt.xlabel("Width")
  plt.ylabel("Height")
  plt.title("Image " + path)
 
plot_image(imagepaths[0]) #We plot the first image from our imagepaths array
 
X = [] # Image data
y = [] # Labels
 
# Loops through imagepaths to load images and labels into arrays
for path in imagepaths:
  img = cv2.imread(path) # Reads image and returns np.array
  # Converts into the correct colorspace (GRAY)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
  # Reduce image size so training can be faster  
  img = cv2.resize(img, (320, 120))   
  X.append(img)  
  # Processing label in image path
  category = path.split("/")[3]
  # We need to convert 10_down to 00_down, or else it crashes
  label = int(category.split("_")[0][1])   
  y.append(label)
 
# Turn X and y into np.array to speed up train_test_split
X = np.array(X, dtype="uint8")
# Needed to reshape so CNN knows it's different images
X = X.reshape(len(imagepaths), 120, 320, 1) 
y = np.array(y)
 
print("Images loaded: ", len(X))
print("Labels loaded: ", len(y))
 
print(y[0], imagepaths[0]) # Debugging
# 30 % of images are used for testing.
# the rest are used for training
ts = 0.3 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts,\ random_state=42)
# Import of keras model and hidden layers for our convolutional network
from keras.models import Sequential
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.layers import Dense, Flatten
 
# Construction of model
model = Sequential()
model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu')) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))
 
# Configures the model for training
model.compile(optimizer='adam', # Optimization routine, which tells the #computer how to adjust the parameter values to minimize the loss function.
              loss='sparse_categorical_crossentropy', # Loss function, which #tells us how bad our predictions are.
              metrics=['accuracy']) # List of metrics to be evaluated by the #model during training and testing.
 
# Trains the model for a given number of epochs (iterations on a dataset) and # validates it.
model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=2, validation_data=(X_test, y_test))
 
# Save entire model to a HDF5 file
model.save('handrecognition_model.h5')
test_loss, test_acc = model.evaluate(X_test, y_test)
 
print('Test accuracy: {:2.2f}%'.format(test_acc*100))
predictions = model.predict(X_test) # Make predictions towards the test set
# Transform predictions into 1-D array with label number
y_pred = np.argmax(predictions, axis=1) 
# H = Horizontal
# V = Vertical
 
pd.DataFrame(confusion_matrix(y_test, y_pred), 
             columns=["Predicted Thumb Down", "Predicted Palm (H)", "Predicted L", "Predicted Fist (H)", "Predicted Fist (V)", "Predicted Thumbs up", "Predicted Index", "Predicted OK", "Predicted Palm (V)", "Predicted C"],
             index=["Actual Thumb Down", "Actual Palm (H)", "Actual L", "Actual Fist (H)", "Actual Fist (V)", "Actual Thumbs up", "Actual Index", "Actual OK", "Actual Palm (V)", "Actual C"])
